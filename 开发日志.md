# 开发者日志

## 背景
        一边学习boost context 和 一些开源协程框架。自己也想从诸多协程框架中选一个，自己实现一下。然后在写一个rpc协议。


### 6月1日
        学习mmap、mportect等系统调用，自实现内存申请和释放。
        
        通过mmap实现的内存释放和申请，实现 YRoutineStack(协程栈空间) 
        
        学习boost context detail里面的几个接口：make_fcontext、jump_fcontext，封装Yield操作和Resume。真的很方便诶。
    不用去思考那些复杂的汇编调用、跨平台（虽然也看了一下汇编代码大致做了什么就是说），一键自动切换。
        
        封装boost fcontext_t 实现的协程上下文。

### 6月2日
                踩坑：jump_fcontext 返回值是来源上下文，所以需要想个办法让下一个协程主函数去做把，协程栈切换。不然就很麻烦
                GetCurrentContext() = boost::context::detail::jump_fcontext(context_,reinterpret_cast<void*>(this)).fctx;
        这一句应该是保存当前上下文，并切换到下一个上下文，但是jump_fcontext尚未保存。导致TLS变量未记录下来主协程的上下文

                就非常的奇怪，主要还是因为协程上下文切换，会连带这站空间一起切换。导致比较繁琐的问题。boost的用返回值去解决协程中
        主函数跳转问题。所以需要有个同步变量解决yield和resume跳转不同步的问题。

### 6月3日
                所以改变思路，参考 phxrpc 中的协程实现。使用主协程调度+子协程执行的方式。这样我们yield只会由子协程调用。那么返
        回之后，只需要保存来源上下文（来源必定是主协程）即可（通过jump_fcontext传参的方式）。同时resume只会由主协程（main调度协程）
        执行调用

### 6月4日      
                测试YRoutineScheduler，发现Scheduler里面有一个不细心的错误。因为对于上下文切换不是非常熟练，所以上下文切换前后到底谁
        拥有线程控制权，不是很熟悉。导致我有些地方，还以为就像正常函数一样顺序执行。

        比如
                Coroutinedone执行中，我需要更新这个协程状态
                错误写法:
                {
                        Routine.Yield();        //此时已经切换走了控制权。所以不会调用到updata去更新信息
                        updata();
                }
                正确写法:
                {
                        updata();
                        Routine.Yield();
                }

                协程部分，需要配合hook（dlsym和dlopen劫持系统调用，替换为自己的实现）使用或者非hook（重载、一套代替函数）
                我个人喜欢用一套代替函数，因为这样可以和系统函数区分开，而不是用 enable_hook_thread();可能个人习惯不同吧。


                准备封装epoll和Scheduler，这样后续才可以hook


### 6月6日
                定时器队列，用的优先级队列，测试案例需要完善
                ok，回归正题，继续想办法封装epoll和hook系统函数

                下午
                修改了timequeue，继续写epoll的接口，看了几个开源库，感觉对于hook的封装都差不太多，思路都是先执行一次，然后
        需要再次执行就添加到epoll，再yield。等epoll那边有消息，回来再resume当前协程。
                现在思路有了，就是epoll要提供什么样的接口，还需要看看。我看phxrpc里面的epollscheduler挺有意思的。准备看一下。


### 6月8日
                其实不是协程框架，就不能hook，倒也不是不能，因为hook会污染源代码（可以用enable hook标志位解决，但是每个线程
        都要有一个TLS，也是一种损失。


### 6月11日     
                测试例程编写，测试例程发现bug。
                cpu占用率极高，但是感觉大概率是虚拟机的问题。因为一个while循环就100%。但是奇怪的地方在于我编写的测试例程epollwait
        10ms 占用率极低(0.3)。但是确实很奇怪的样子。
                目前看来不会有什么问题，就是单线程单协程，在loop中就会出现这样的问题。

### 6月12日
                调研protobuf，设计协议体。
                了解protobuf中c++的反射机制。学习c++17的any（万能容器）和 c++14的string_view
                代码没写几行，剩下开始就是自己重新设计了。

### 6月13日
                今天任务基本完成了，剩下就是写codec和Servicemap。说实话，我困惑的一点就是使用者在使用肯定会定义不同的服务，需要的
        参数和数据也各不相同，怎么在不知道用户会定义什么服务的情况下，保存和映射就是一个难点。
                准备使用define创建一个allservice类，让使用者在使用之前，就定义所有类的一个空实现，这样可以实现类型推断，和Service
        工厂

### 6月15日
                简简单单封装一个协程网络框架，就在yrpc::ynet 命名空间里面。完全是基于YRoutine 和 Epoller的。后续用于封装RpcServer
        和RpcClient。
                需要通过协程封装的hook 去新建一个 network。和libyqmnet不适配的原因就是在这里，因为yrpc是一个基于协程的rpc。所以需
        要根据协程框架去修改原本的reactor模型。但是本质上还是一个依赖于 reactor 事件驱动的协程调度。所以在测试之前不好说有多大提升
                现在需要考虑如何去注册统一的rpc服务解决接口。因为在listen、accept后。连接建立，到传输信息，处理服务，返回结果。这里
        应该是属于rpc的主体逻辑部分了。当前先设置为待定的接口。


### 6月16日
                每个connection，是否需要output和input？协程环境也需要吗？其实就看有没有阻塞的需求。阻塞了，就算是协程环境，也是需要
        按照当前协程的任务继续往下走，其实可以说我写的这种协程调度模式完全就是和reactor模式的Eventloop一致。协程切换也只是在于是不是
        主动阻塞（yield），和被动阻塞（read、write、锁）。所以输入输出缓冲依旧需要。缓冲区，先不重写，暂时沿用yqmnet中的Buffer。
                尽快完成network部分，但是当前设计会导致network和rpc部分耦合比较严重。


### 6月17日
                需要设计一个类，这个类的功能应该是这样的。输入protobuf未解析数据，自动解析并找到对应服务，调用服务并生成打包序列化一
        个响应protobuf（不论成功还是失败）。
                1、接受protobuf原始数据，传出protobuf序列化数据。
                2、解析protobuf、生成指定的protobuf。
                3、调用服务。
                4、作为一个Service的管理者。
                
                需要解耦，把msg处理统一包装成一个class。而不是现在的msg处理的不同部分都散落在网络连接的不同步骤中。

### 6月18日
                写单元测试、Caller和服务匹配模块

### 6月19日
                先实现短连接，后续更新为连接池
                连接池
                （一）一次call独占连接直到返回，应该是维持多个长链接，但是就像线程一样，一次只能处理一个连接。这种情况，设计的复杂在
        客户端，因为连接池维持在客户端，对于服务端来说，不可见（每条连接都是相对独立的，不必要集中管理）。
                （二）第二种就是长链接可以同时处理多个任务。但是这种需要标识服务，建立req和rsp的映射关系。逻辑上相对复杂，而且会导致服务
        端压力较大（同时执行任务数会多于连接数）
                
                当前：短连接
                一个连接建立处理一个任务，处理完返回即断开连接。

### 6月20日
                Rpc服务端基本逻辑完成，先写玩Rpc客户端，再测试debug。当前肯定有bug，但是需要测试看看是什么原因。


### 6月22日
                RpcServer和RpcClient封装完毕。
                todo
                        Cond_t: pipe实现,依靠epoll
                        YRSleep: 不能阻塞协程，但是可以让出协程的机制。决定采用pipe实现一个Cond_t 
                        优化Cond
                        提供更多协程工具。
                        优化Epoller接口和内部接口。
                        测试和Debug
                        

### 6月23日
                客户端的设计上有逻辑上的问题，当前是直接执行，再去调用Epoller的Loop，但是直接执行，就会在主协程中开始行动，其实还是要注册回调
        在主协程中注册子协程再处理事件。但是当前提供的同步接口不可行，所以需要Future或者callback，决定两个都试一下，但是先实现callback回调模型       


### 6月24日
                epoll_wait 异常触发 connect。


### 6月25日
                昨天遇到的connect问题，就是普通的连接被拒绝，这次学到了，要通过getsockopt去判断错误原因，不然就会出错。
                新增bug：connect send失败

                今天散步，发现一个重要的问题，为什么client要在主线程pool，其实完全可以提供同步和异步接口，但是需要把操作放在后台。这样的话就会
        更容易实现同步调用。

### 6月26日
                RpcServer和RpcClient测试案例通过。但是当前信号量跨线程唤醒次序还存在问题。不过可以正式宣布 yrpc 第一版成功！

                接下来花1周去debug。


### 6月27日
                待优化、待实现内容：
                
                1、RpcClient 连接池，针对调用频率较高的任务使用长链接，并抽象出调用包，根据调用包在连接池中处理，处理完返回结果，并根据调用包通知
        调用方者（提供异步接口和future接口）。（ps: 如果使用连接池，是否采取一次调用独占一个连接？还是采取调用信息发送，等待调用结果返回并匹配对应
        的调用包，再通过调用包通知调用者。后者应该需要一个调用包通知管理器，它的作用就是设置客户端调用超时和调用与调用方的映射关系）
                2、RpcServer 错误验证和错误处理，当前只是实现功能，但是对于调用超时和错误处理没有做详细的设计，后续将添加错误码和错误工具。提高
        Server端健壮性。
                3、RpcClient 异步接口的验证，当前只是验证了future接口的可行性。需要编写async接口的测试案例并debug。
                


### 6月30日
                长连接+复用。发送call，客户端分配id，然后服务端线程池处理完毕发送。
                


### 7月4日
                目前客户端和服务端解析有问题，大概率是buffer的问题。等待优化

### 7月5日
                还是客户端和服务端报文解析有问题，但是还没有定位是哪个模块出问题了。


### 7月7日
                服务端积包和解包实现完成，单机 qps 提高至 8300左右(上下浮动500)


### 8月10日     
                实习中，抽时间，继续更新
                        issue：
                                1、重构 + Debug 
                                        (1). 目前错误处理不是很明确，比如服务端任务失败返回给客户端是什么样的错误码？需要分析各种错误情况
                                        并考虑错误码，不然调试起来会很麻烦，不利于寻找问题的根源。
                                        
                                        (2). RpcServer端，对于一个任务的处理，如果是一个需要耗费很多资源的任务，但是客户端断开了，就是不需要了
                                        这个时候服务端完成后数据是否缓存（比如保存10s，如果客户端因为网络波动掉线重连，就可以不用耗费资源去再
                                        调用一次服务了），所以可能需要在协议上加上这个控制信息。
                                        
                                <优先>  (3). KeepALive机制，现在还没有心跳机制，需要修改 network 部分，使 connection 提供 heart beat 机制。
                                        
                                        (4). 代码优化，现在的代码比较丑陋，而且没有对接口进行很好的分装，我喜欢提供给用户，就只暴露服务调用接
                                        口和控制信息。就像 setsocketopt、 getsocketopt 那种接口风格。尽量少使用宏，易读性需要强一点。
                                        
                                <优先>  (5). RpcServer端，好像有个bug，多Client连接会导致SEGV。尚不清楚原因。


                                2、提供 HTTP parse 库
                                        去网上找一个抄一下就行了
                                        
                                3、提供 SSL 支持
                                        可能需要重新封装 network 部分，将SSL SOCKET 作为选项。

                


### 8月28日
        服务端如何处理任务？
        
        为了让服务端任务是并发处理，我在服务端设置了一个线程池，因为线程的调度是由系统进行的，任务处理较为均匀。本身没想到更好的处理方法。而服务端的ServerSingle内部只有一个线程和一个线程池，main线程其实是跑协程的，一连接一协程，这个暂时还没有设置单线程内部可以开多少个线程。（todo）
                 


### 9月4日
        干掉服务端隐形的线程池，速度显著提升，每秒2w qps 无压力（test 程序echo）.现在设计思路改了，线程池不再是RpcServer提供而是让用户自己在Callback中封装。感觉从库设计上来说更好，因为这样用户可以有更高的自由度，如果默认提供线程池实现异步调用，不一定是用户想要的。


### 9月10日
        完善自定义协议。计划统一自定义协议，格式如下：
                |自定义协议头|protobuf|
        需要考虑如下问题：粘包、分包，protocol type。计划将错误码以enum形式确定下来，部分错误信息如：丢失、错误、超时、心跳等信息都包含进去。

### 9月11日
        完成了新的IDGenerate，32位uid，10分钟内绝对唯一。64位uid，24小时内绝对唯一。考虑一下uuid和雪花算法.

### 9月12日
        完成了自定义协议模块，准备添加协议和协议处理handler.走一个基于事件的。


### 9月17日
        将Service、ServerSingle部分整合为RpcServerSession，CallManager整合为RpcClientSession.


### 9月25日
        需要测试收发协议。或者写个协议捕获工具。看情况吧。目前是，收发存在问题，国庆细化日志看一看问题。